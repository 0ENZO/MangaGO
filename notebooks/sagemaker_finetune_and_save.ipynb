{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "_*Note:* we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you havenÂ´t it installed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "ipywidgets\n",
    "git+https://github.com/huggingface/transformers\n",
    "datasets\n",
    "sacrebleu\n",
    "torch\n",
    "sentencepiece\n",
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install -r colab_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas==1.5.2\n",
    "# s3fs==2022.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::984909470121:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole\n",
      "sagemaker bucket: mangago-dataset\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket='mangago-dataset'\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "bucket ='mangago-dataset'\n",
    "folder = 'raw'\n",
    "data_key = 'deepl_translated_df_47.csv'\n",
    "data_location = 's3://{}/{}/{}'.format(bucket, folder, data_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://mangago-dataset/raw/deepl_translated_df_47.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('s3://mangago-dataset/raw/deepl_translated_df_47.csv', sep=';', encoding='utf-8')\n",
    "df = df.rename(columns={'text':'ja','translation':'fr'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/ec2-user/.cache/huggingface/datasets/csv/default-7903a7d381a39996/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b0b2e983b94c5aa729f754abff1ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/ec2-user/.cache/huggingface/datasets/csv/default-7903a7d381a39996/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-a8fbf5b04ded70ea.arrow and /home/ec2-user/.cache/huggingface/datasets/csv/default-7903a7d381a39996/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-5fb917b197b8cd09.arrow\n",
      "Loading cached split indices for dataset at /home/ec2-user/.cache/huggingface/datasets/csv/default-7903a7d381a39996/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-9e7a853b76bc146c.arrow and /home/ec2-user/.cache/huggingface/datasets/csv/default-7903a7d381a39996/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-11c934a67949fffd.arrow\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_metric\n",
    "\n",
    "csv_dataset = datasets.load_dataset(\"csv\", data_files=data_location, keep_default_na=False, delimiter=';')\n",
    "dataset = csv_dataset['train']\n",
    "\n",
    "# dataset = Dataset.from_pandas(df)\n",
    "\n",
    "dataset = dataset.rename_columns({'text': 'ja', 'translation': 'fr'})\n",
    "\n",
    "# 90% train, 10% (test + validation)\n",
    "dataset_train_test = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# 10% to 5% test + 5% validation\n",
    "dataset_test_valid = dataset_train_test['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': dataset_train_test['train'],\n",
    "    'test': dataset_test_valid['test'],\n",
    "    'validation': dataset_test_valid['train']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_name = 'Helsinki-NLP/opus-mt-ja-fr'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\n",
    "max_input_length = 256\n",
    "max_target_length = 256\n",
    "source_lang = \"ja\"\n",
    "target_lang = \"fr\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex for ex in examples[\"ja\"]]\n",
    "    targets = [ex for ex in examples[\"fr\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/csv/default-7903a7d381a39996/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-919eb33527a1168a.arrow\n",
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/csv/default-7903a7d381a39996/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-6db5a8cd7dd3a702.arrow\n",
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/csv/default-7903a7d381a39996/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-6fb7bcc646b8cc32.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "save_path = f's3://{bucket}/preprocessed/dataset-{datetime.now().strftime(\"%d-%m-%Y-H-M\")}'\n",
    "tokenized_datasets.save_to_disk(save_path, fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/ec2-user/.cache/huggingface/datasets/csv/default-7903a7d381a39996/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-c1319dc038baae5c.arrow\n",
      "Loading cached shuffled indices for dataset at /home/ec2-user/.cache/huggingface/datasets/csv/default-7903a7d381a39996/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-972139eecc90b079.arrow\n"
     ]
    }
   ],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be839db96314dcca1ad25170a3b33eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/306M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7760ac6fa10448f89ff92ebb93c3f802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â¦)neration_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"checkpoint-36500\", from_tf=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-ja-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "batch_size = 16\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"model-finetuned-{datetime.now().strftime('%d-%m-%Y')}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=7,\n",
    "    save_strategy = \"no\",\n",
    "    load_best_model_at_end=False,\n",
    "    predict_with_generate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7418/365506015.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ð¤ Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61eb51cb5f646c5a5d68c1e862f12c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.85k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 11:28, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.125080</td>\n",
       "      <td>5.366800</td>\n",
       "      <td>27.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.082277</td>\n",
       "      <td>6.326300</td>\n",
       "      <td>27.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.066849</td>\n",
       "      <td>7.843300</td>\n",
       "      <td>24.103000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.060434</td>\n",
       "      <td>9.301800</td>\n",
       "      <td>22.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.058435</td>\n",
       "      <td>9.164400</td>\n",
       "      <td>22.055000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=315, training_loss=1.6601982964409723, metrics={'train_runtime': 688.3966, 'train_samples_per_second': 7.263, 'train_steps_per_second': 0.458, 'total_flos': 34678027321344.0, 'train_loss': 1.6601982964409723, 'epoch': 5.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "output_model_name = f\"model_finetuned_{datetime.now().strftime('%d-%m-%Y')}.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model('model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\n",
      "vocab.json\n",
      "pytorch_model.bin\n",
      "source.spm\n",
      "special_tokens_map.json\n",
      "training_args.bin\n",
      "target.spm\n",
      "generation_config.json\n",
      "tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "def compress(tar_dir=None,output_file=output_model_name):\n",
    "    parent_dir=os.getcwd()\n",
    "    os.chdir(tar_dir)\n",
    "    with tarfile.open(os.path.join(parent_dir, output_file), \"w:gz\") as tar:\n",
    "        for item in os.listdir('.'):\n",
    "          print(item)\n",
    "          tar.add(item, arcname=item)\n",
    "    os.chdir(parent_dir)\n",
    "\n",
    "compress(str('model_finetuned/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model uploaded to: s3://mangago-finetuned-models/model_finetuned_02-07-2023//model_finetuned_02-07-2023.tar.gz\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "s3_model_uri = S3Uploader.upload(local_path=output_model_name, desired_s3_uri=f\"s3://mangago-finetuned-models/model_finetuned_{datetime.now().strftime('%d-%m-%Y')}/\")\n",
    "print(f\"model uploaded to: {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('s3://mangago-finetuned-models/model-finetuned-02-07/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ja_valid = [\n",
    "     'ã ããã£',\n",
    "     'ç¥ããªãã£ã¦è¨ã£ã¦ãã ãã£',\n",
    "     'ãããªåéãªãã¦!',\n",
    "     'ããã¯è¨ã£ã¦ããªã',\n",
    "     'ãã£ã¡ã«ãåç¨æ¸ãããã ã',\n",
    "     'ãã«ãã£ã¤ã¼ãã«åããéã¯ã¡ããã¨è¿ãã¦è²°ãã­ãã¨',\n",
    "     'ç¥ããããã£',\n",
    "     'ç¶è¦ªãã«ã¸ãã§ä½ã£ãåéãªãã¦...',\n",
    "     'ããããã«ãã£ã¤ã¼ãä¸å®¶ã®æ¹ã§ãããã',\n",
    "     'åãè¾¼ã¿ä¸­ã åºç´ãããã!',\n",
    "     'å¾ã£ã¦ãã ãã¼ã¼',\n",
    "     'ããã¾ãã...ã£ç®ã®è¦ããªããéãç¼ããªãã®ã§ã¤ãè»¢ãã§ããã£ã¦',\n",
    "     'éãç¼ãããã£ã¢ã«ä½ã®ç¨ã ã³ã©ã¡!!',\n",
    "     'æµãã®ã¬ã³ãã³ããã¦ããã·ãªã©ã¨ç³ãã¾ã',\n",
    "     'ãããä¸å¸¯ãä»åããã«ãã£ã¤ã¼ä¸å®¶ã®åãèãããã®ã§',\n",
    "     'ç¨å¿æ£ã¨ãã¦éã£ã¦ããã ããªããã¨...',\n",
    "     'ç®ã®è¦ããªãéãç¼ã...',\n",
    "     'ã¬ã³ãã³ã ã£ã¦...?',\n",
    "     'ããã£ã¢èããã®ãå¤§æ¦ã«ãããã',\n",
    "     'éãç¼ãç¨å¿æ£ã ?',\n",
    "     'è°ã®éã§ã¤ãããè³ã¿ãã¶ã¡ã¾ãã¦ã¿ãã?',\n",
    "     'ã¡ãã£ã¨ããã¦ã!ãããªã¨ãã§!',\n",
    "     'ãåããããã¦',\n",
    "     'å¥³ã?',\n",
    "     'ãã£ã¯ã...ããã§ããã©...',\n",
    "     'ã»ã',\n",
    "     'é¢è¦ãã¦ã¿ã',\n",
    "     'éãç¼ã§ãå¨éãè¯ãããè²¿ãæã¯...',\n",
    "     'ãã£ã¡ãã£ã¨...!',\n",
    "]\n",
    "\n",
    "fr_valid = [\n",
    "     'Je vous le dis !',\n",
    "     'Je ne sais pas de quoi vous parlez !',\n",
    "     'je ne vous dois rien !',\n",
    "     'Eh bien, je suis dÃ©solÃ©...',\n",
    "     'nous avons des preuves ici...',\n",
    "     'Vous savez que nous avons besoin que vous nous remboursiez...',\n",
    "     'Comment le saurais-je ?',\n",
    "     'Comment puis-je connaÃ®tre la dette de mon pÃ¨re dans les casinos...',\n",
    "     'Vous Ãªtes membre de la famille Tortillano ?',\n",
    "     'nous avons une conversation ici ! Perdez la tÃªte !',\n",
    "     \"S'il vous plaÃ®t... ! Attendez...\",\n",
    "     'Je suis dÃ©solÃ©, je ne vois rien, je suis un \"Åil fermÃ©\"...',\n",
    "     \"Alors, qu'est-ce qu'un Åil fermÃ© attend de nous ?\",\n",
    "     'Je suis Siora, un tireur',\n",
    "     \"J'ai entendu dire que c'est vous, les Tortillanos, qui Ãªtes aux commandes ici.\",\n",
    "     \"Je me demandais si vous pouviez m'engager comme garde du corps...\",\n",
    "     'donc un Åil fermÃ© est...',\n",
    "     'un tireur... ?',\n",
    "     'Ne vous moquez pas de nous.',\n",
    "     'yeux fermÃ©s pour un garde du corps ?',\n",
    "     'Tu veux que je te tire une balle dans la tÃªte ? !',\n",
    "     \"ArrÃªtez, s'il vous plaÃ®t !\",\n",
    "     'Ãªtes-vous...',\n",
    "     'une fille ?',\n",
    "     'oui, mais alors... ?',\n",
    "     'oh bien sÃ»r...',\n",
    "     'Regardons votre visage.',\n",
    "     'Si vous avez de bonnes qualitÃ©s, vos yeux ne sont pas un problÃ¨me.',\n",
    "     'HÃ©, attendez !',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f34c42e67bf4583b51d0d91a8fc4bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â¦)lve/main/config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76dd30a364a4455a40ac709f32c4942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/306M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672f6bf57247449999e8d5a79dc318ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â¦)neration_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b381ea04c86e46f3855a7be03358c72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â¦)okenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0aef9af197d41b19cf328efaaa24b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â¦)olve/main/source.spm:   0%|          | 0.00/788k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff38e45b2ef4e619ab66ab88e4a8a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â¦)olve/main/target.spm:   0%|          | 0.00/828k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a1bea6997d4259896eeb8607a356b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â¦)olve/main/vocab.json:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Fais-moi voir.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation\", model='Helsinki-NLP/opus-mt-ja-fr')\n",
    "translator('é¢è¦ãã¦ã¿ã')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Si le volume de l'Åil est bon, il y a une main lourde...\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model_name = 'model_nul/'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('model_nul/')\n",
    "\n",
    "translated = model.generate(**tokenizer('éãç¼ã§ãå¨éãè¯ãããè²¿ãæã¯...', return_tensors=\"pt\", padding=True), repetition_penalty=10.)\n",
    "[tokenizer.decode(t, skip_special_tokens=True) for t in translated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model_name = 'Helsinki-NLP/opus-mt-ja-fr'\n",
    "finetuned_model_name = 'model_nul/'\n",
    "\n",
    "basic_model = AutoModelForSeq2SeqLM.from_pretrained(basic_model_name)\n",
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(finetuned_model_name)\n",
    "\n",
    "basic_tokenizer = AutoTokenizer.from_pretrained(basic_model_name)\n",
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(finetuned_model_name)\n",
    "    \n",
    "basic_generation = basic_model.generate(**tokenizer(ja_valid[:10], return_tensors=\"pt\", padding=True), repetition_penalty=10.)\n",
    "basic_translations = [basic_tokenizer.decode(t, skip_special_tokens=True) for t in basic_generation]\n",
    "\n",
    "finetuned_generation = finetuned_model.generate(**tokenizer(ja_valid[:10], return_tensors=\"pt\", padding=True), repetition_penalty=10.)\n",
    "finetuned_translations = [finetuned_tokenizer.decode(t, skip_special_tokens=True) for t in finetuned_generation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"C'est pour Ã§a que...\",\n",
       " \"Je te l'ai dit, je ne sais pas.\",\n",
       " \"Je n'arrive pas Ã  croire que tu me dois Ã§a!\",\n",
       " \"Je n'ai pas dit Ã§a.\",\n",
       " \"J'ai une carte de crÃ©dit.\",\n",
       " \"Il a dit qu'il n'avait pas besoin de rÃ©cupÃ©rer l'argent que j'avais empruntÃ© Ã  Trujano.\",\n",
       " 'Je ne sais pas.',\n",
       " 'Je ne peux pas croire que mon pÃ¨re ait crÃ©Ã© une dette dans un casino.',\n",
       " \"C'est la famille Turtiliano.\",\n",
       " \"Je suis en train de m'en occuper. Rebrousse-la!\"]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"C'est pour cela que...\",\n",
       " 'Je vous dis que je ne sais pas.',\n",
       " \"Je n'arrive pas Ã  croire que vous ayez une telle dette!\",\n",
       " \"Je n'ai pas dit Ã§a.\",\n",
       " \"J'ai un compte de dÃ©pÃ´t ici.\",\n",
       " \"L'argent que j'ai prÃªtÃ© Ã  Trujano, je dois le rendre.\",\n",
       " 'Je ne sais pas.',\n",
       " \"Je n'arrive pas Ã  croire que mon pÃ¨re ait crÃ©Ã© une dette dans un casino.\",\n",
       " \"Si c'est le cas, est-ce la famille Turtiliano?\",\n",
       " 'Je suis en train de le faire. RÃ©pare-toi!']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic_model_bleu: 0.46411279345926965\n",
      "finetuned_model_bleu: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "basic_model_bleu = corpus_bleu(basic_translations, fr_valid[:10])\n",
    "finetuned_model_bleu = corpus_bleu(finetuned_translations, fr_valid[:10])\n",
    "\n",
    "print(\"basic_model_bleu:\", basic_model_bleu.score)\n",
    "print(\"finetuned_model_bleu:\", finetuned_model_bleu.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Je vous le dis !',\n",
       " 'Je ne sais pas de quoi vous parlez !',\n",
       " 'je ne vous dois rien !',\n",
       " 'Eh bien, je suis dÃ©solÃ©...',\n",
       " 'nous avons des preuves ici...',\n",
       " 'Vous savez que nous avons besoin que vous nous remboursiez...',\n",
       " 'Comment le saurais-je ?',\n",
       " 'Comment puis-je connaÃ®tre la dette de mon pÃ¨re dans les casinos...',\n",
       " 'Vous Ãªtes membre de la famille Tortillano ?',\n",
       " 'nous avons une conversation ici ! Perdez la tÃªte !']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_valid[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLEU = 0.00 0.0/0.0/0.0/0.0 (BP = 1.000 ratio = 9.400 hyp_len = 94 ref_len = 10)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bleu(finetuned_translations, fr_valid[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLEU = 0.46 1.1/0.6/0.3/0.2 (BP = 1.000 ratio = 9.200 hyp_len = 92 ref_len = 10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bleu(basic_translations, fr_valid[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d146e6016c4b15b22e12577ddff35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5bd95b5f94c483388e92c0a80d6ba60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9c61583fa548159a993d454a7a63d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [0.22340425531914893, 0.05952380952380952, 0.013513513513513514, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0930232558139534, 'translation_length': 94, 'reference_length': 86}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "print(bleu.compute(predictions=finetuned_translations, references=fr_valid[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(bleu.compute(predictions=basic_translations, references=fr_valid[:10])['bleu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Je vous le dis !',\n",
       " 'Je ne sais pas de quoi vous parlez !',\n",
       " 'je ne vous dois rien !',\n",
       " 'Eh bien, je suis dÃ©solÃ©...',\n",
       " 'nous avons des preuves ici...',\n",
       " 'Vous savez que nous avons besoin que vous nous remboursiez...',\n",
       " 'Comment le saurais-je ?',\n",
       " 'Comment puis-je connaÃ®tre la dette de mon pÃ¨re dans les casinos...',\n",
       " 'Vous Ãªtes membre de la famille Tortillano ?',\n",
       " 'nous avons une conversation ici ! Perdez la tÃªte !']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_valid[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"C'est pour Ã§a que...\",\n",
       " \"Je te l'ai dit, je ne sais pas.\",\n",
       " \"Je n'arrive pas Ã  croire que tu me dois Ã§a!\",\n",
       " \"Je n'ai pas dit Ã§a.\",\n",
       " \"J'ai une carte de crÃ©dit.\",\n",
       " \"Il a dit qu'il n'avait pas besoin de rÃ©cupÃ©rer l'argent que j'avais empruntÃ© Ã  Trujano.\",\n",
       " 'Je ne sais pas.',\n",
       " 'Je ne peux pas croire que mon pÃ¨re ait crÃ©Ã© une dette dans un casino.',\n",
       " \"C'est la famille Turtiliano.\",\n",
       " \"Je suis en train de m'en occuper. Rebrousse-la!\"]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
